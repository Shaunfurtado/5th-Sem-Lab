{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Astar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aStarAlgo(start_node,stop_node):\n",
    "    open_set=set(start_node)\n",
    "    closed_set=set()\n",
    "    g = {}\n",
    "    parents = {}\n",
    "    g[start_node] = 0\n",
    "    parents[start_node] = start_node\n",
    "    \n",
    "    while len(open_set) > 0:\n",
    "        n=None\n",
    "        for v in open_set:\n",
    "            if n == None or g[v] + heuristic(v) < g[n] +  heuristic(n):\n",
    "                n = v\n",
    "                \n",
    "        if n==stop_node or Graph_nodes[n] == None:\n",
    "            pass\n",
    "        else:\n",
    "            for (m,  weight) in get_neighbors(n):\n",
    "                \n",
    "                if m not in open_set and m not in closed_set:\n",
    "                    open_set.add(m)\n",
    "                    parents[m] = n\n",
    "                    g[m] = g[n] + weight\n",
    "                else:\n",
    "                    if g[m] > g[n]+weight:\n",
    "                        g[m]=g[n]+weight\n",
    "                        parents[m]=n \n",
    "                        if m in closed_set:\n",
    "                            closed_set.remove(m)\n",
    "                            open_set.add(m)\n",
    "        if n==None:\n",
    "            print('path does not exist')\n",
    "            return None\n",
    "        if n == stop_node:\n",
    "            path=[]\n",
    "            while parents[n] !=n:\n",
    "                path.append(n)\n",
    "                n=parents[n]\n",
    "            path.append(start_node)\n",
    "            path.reverse()\n",
    "            print('Path found: {}'.format(path))\n",
    "            return path\n",
    "        open_set.remove(n)\n",
    "        closed_set.add(n)\n",
    "    print('path does not exist!')\n",
    "    return None\n",
    "\n",
    "def get_neighbors(v):\n",
    "            if v in Graph_nodes:\n",
    "                return Graph_nodes[v] \n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "def heuristic(n):\n",
    "    H_dist={\n",
    "        'A':11,\n",
    "        'B':6,\n",
    "        'C':99,\n",
    "        'D':1,\n",
    "        'E':7,\n",
    "        'G':0\n",
    "    }     \n",
    "    return H_dist[n]\n",
    "\n",
    "Graph_nodes={\n",
    "    'A':[('B',2),('E',3)],\n",
    "    'B':[('G',9),('C',1)],\n",
    "    'E':[('D',6)],\n",
    "    'D':[('G',1)]\n",
    "}\n",
    "aStarAlgo('A','G')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Best First Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "Graph_nodes = { \n",
    "   'S': ['A','B'], \n",
    "   'A': ['C','D'], \n",
    "   'B': ['E','F'], \n",
    "   'E': ['H'],\n",
    "   'F': ['I','G']\n",
    "   }\n",
    "def heuristic(n):\n",
    "    H_dist = { \n",
    "        'S': 13,\n",
    "        'A': 12,\n",
    "        'B': 4,\n",
    "        'C': 7,\n",
    "        'D': 3,\n",
    "        'E': 8,\n",
    "        'F': 2,\n",
    "        'H': 4,\n",
    "        'I': 9,\n",
    "        'G': 0\n",
    "     } \n",
    "    return H_dist[n] \n",
    "\n",
    "def best_first_search(graph, start, goal):\n",
    "    \"\"\"\n",
    "    graph: Dict having Tree Structure.\n",
    "    start: Starting Node of the Tree.\n",
    "    goal: Node to Search.\n",
    "    heuristic: List of heuristic cost.\n",
    "    \"\"\"\n",
    "    open = [(heuristic(start), start)]\n",
    "    closed = {}\n",
    "    closed[start] = None\n",
    "    \n",
    "    while open:\n",
    "        _,peak_node = heapq.heappop(open)\n",
    "        \n",
    "        if peak_node == goal:\n",
    "            break\n",
    "            \n",
    "        for neighbor in graph[peak_node]:\n",
    "            if neighbor not in closed:\n",
    "                heapq.heappush(open, (heuristic(neighbor), neighbor))\n",
    "                closed[neighbor] = peak_node\n",
    "    \n",
    "    return closed\n",
    "start_node = 'S'\n",
    "goal_node = 'G'\n",
    "closed = best_first_search(Graph_nodes, start_node, goal_node)\n",
    "print(\"Closed list\",closed)\n",
    "node = goal_node\n",
    "path = [node]\n",
    "\n",
    "while node != start_node:\n",
    "    node = closed[node]\n",
    "    path.append(node)\n",
    "    \n",
    "path.reverse()\n",
    "print(\"BFS path from\",start_node,\"to\",goal_node,\":\",path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.AOStar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(self, graph, heuristicNodeList, startNode):  #instantiate graph object with graph topology, heuristic values, start node\n",
    "        \n",
    "        self.graph = graph\n",
    "        self.H=heuristicNodeList\n",
    "        self.start=startNode\n",
    "        self.parent={}\n",
    "        self.status={}\n",
    "        self.solutionGraph={}\n",
    "     \n",
    "    def applyAOStar(self):         # starts a recursive AO* algorithm\n",
    "        self.aoStar(self.start, False)\n",
    "\n",
    "    def getNeighbors(self, v):     # gets the Neighbors of a given node\n",
    "        return self.graph.get(v,'')\n",
    "    \n",
    "    def getStatus(self,v):         # return the status of a given node\n",
    "        return self.status.get(v,0)\n",
    "    \n",
    "    def setStatus(self,v, val):    # set the status of a given node\n",
    "        self.status[v]=val\n",
    "    \n",
    "    def getHeuristicNodeValue(self, n):\n",
    "        return self.H.get(n,0)     # always return the heuristic value of a given node\n",
    " \n",
    "    def setHeuristicNodeValue(self, n, value):\n",
    "        self.H[n]=value            # set the revised heuristic value of a given node \n",
    "        \n",
    "    \n",
    "    def printSolution(self):\n",
    "        print(\"FOR GRAPH SOLUTION, TRAVERSE THE GRAPH FROM THE START NODE:\",self.start)\n",
    "        print(\"------------------------------------------------------------\")\n",
    "        print(self.solutionGraph)\n",
    "        print(\"------------------------------------------------------------\")\n",
    "    \n",
    "    def computeMinimumCostChildNodes(self, v):  # Computes the Minimum Cost of child nodes of a given node v     \n",
    "        minimumCost=0\n",
    "        costToChildNodeListDict={}\n",
    "        costToChildNodeListDict[minimumCost]=[]\n",
    "        flag=True\n",
    "        for nodeInfoTupleList in self.getNeighbors(v):  # iterate over all the set of child node/s\n",
    "            cost=0\n",
    "            nodeList=[]\n",
    "            for c, weight in nodeInfoTupleList:\n",
    "                cost=cost+self.getHeuristicNodeValue(c)+weight\n",
    "                nodeList.append(c)\n",
    "            \n",
    "            if flag==True:                      # initialize Minimum Cost with the cost of first set of child node/s \n",
    "                minimumCost=cost\n",
    "                costToChildNodeListDict[minimumCost]=nodeList      # set the Minimum Cost child node/s\n",
    "                flag=False\n",
    "            else:                               # checking the Minimum Cost nodes with the current Minimum Cost   \n",
    "                if minimumCost>cost:\n",
    "                    minimumCost=cost\n",
    "                    costToChildNodeListDict[minimumCost]=nodeList  # set the Minimum Cost child node/s\n",
    "                \n",
    "              \n",
    "        return minimumCost, costToChildNodeListDict[minimumCost]   # return Minimum Cost and Minimum Cost child node/s\n",
    "                     \n",
    "    \n",
    "    def aoStar(self, v, backTracking):     # AO* algorithm for a start node and backTracking status flag\n",
    "                print(\"HEURISTIC VALUES  :\", self.H)\n",
    "                print(\"SOLUTION GRAPH    :\", self.solutionGraph)\n",
    "                print(\"PROCESSING NODE   :\", v)\n",
    "                print(\"-----------------------------------------------------------------------------------------\")\n",
    "        \n",
    "                if self.getStatus(v) >= 0:        # if status node v >= 0, compute Minimum Cost nodes of v\n",
    "                    minimumCost, childNodeList = self.computeMinimumCostChildNodes(v)\n",
    "                    self.setHeuristicNodeValue(v, minimumCost)\n",
    "                    self.setStatus(v,len(childNodeList))\n",
    "            \n",
    "                    solved=True                   # check the Minimum Cost nodes of v are solved   \n",
    "                    for childNode in childNodeList:\n",
    "                        self.parent[childNode]=v\n",
    "                        if self.getStatus(childNode)!=-1:\n",
    "                            solved=solved & False\n",
    "            \n",
    "                    if solved==True:             # if the Minimum Cost nodes of v are solved, set the current node status as solved(-1)\n",
    "                        self.setStatus(v,-1)    \n",
    "                        self.solutionGraph[v]=childNodeList # update the solution graph with the solved nodes which may be a part of solution  \n",
    "            \n",
    "            \n",
    "                    if v!=self.start:           # check the current node is the start node for backtracking the current node value    \n",
    "                        self.aoStar(self.parent[v], True)   # backtracking the current node value with backtracking status set to true\n",
    "                \n",
    "                    if backTracking==False:     # check the current call is not for backtracking \n",
    "                        for childNode in childNodeList:   # for eachhttp://localhost:8888/notebooks/aiml/data/Untitled.ipynb?kernel_name=python3# Minimum Cost child node\n",
    "                            self.setStatus(childNode,0)   # set the status of child node to 0(needs exploration)\n",
    "                            self.aoStar(childNode, False) # Minimum Cost child node is further explored with backtracking status as false\n",
    "            \n",
    "        \n",
    "\n",
    "h1 = {'A': 1, 'B': 6, 'C': 2, 'D': 12, 'E': 2, 'F': 1, 'G': 5, 'H': 7, 'I': 7, 'J': 1, 'T': 3}\n",
    "graph1 = {\n",
    "    'A': [[('B', 1), ('C', 1)], [('D', 1)]],\n",
    "    'B': [[('G', 1)], [('H', 1)]],\n",
    "    'C': [[('J', 1)]],\n",
    "    'D': [[('E', 1), ('F', 1)]],\n",
    "    'G': [[('I', 1)]]   \n",
    "}\n",
    "\n",
    "G1= Graph(graph1, h1, 'A')\n",
    "G1.applyAOStar() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.ID3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_tennis = pd.read_csv('4.csv')\n",
    "print(\"\\n Given Play Tennis Data Set:\\n\\n\", df_tennis)\n",
    "\n",
    "def entropy(probs):  \n",
    "    import math\n",
    "    return sum( [-prob*math.log(prob, 2) for prob in probs] )\n",
    "\n",
    "def entropy_of_list(a_list):  \n",
    "    from collections import Counter\n",
    "    cnt = Counter(x for x in a_list)   \n",
    "    num_instances = len(a_list)*1.0   \n",
    "    probs = [x / num_instances for x in cnt.values()]  \n",
    "    return entropy(probs) \n",
    "\n",
    "def information_gain(df, split_attribute_name, target_attribute_name, trace=0):\n",
    "    df_split = df.groupby(split_attribute_name)\n",
    "    nobs = len(df.index) * 1.0\n",
    "    df_agg_ent = df_split.agg({target_attribute_name : [entropy_of_list, lambda x: len(x)/nobs] })[target_attribute_name]\n",
    "    df_agg_ent.columns = ['Entropy', 'PropObservations']\n",
    "    new_entropy = sum( df_agg_ent['Entropy'] * df_agg_ent['PropObservations'] )\n",
    "    old_entropy = entropy_of_list(df[target_attribute_name])\n",
    "    return old_entropy - new_entropy\n",
    "\n",
    "def id3(df, target_attribute_name, attribute_names, default_class=None):\n",
    "    from collections import Counter\n",
    "    cnt = Counter(x for x in df[target_attribute_name])# class of YES /NO\n",
    "    print(cnt)\n",
    "    if len(cnt) == 1:\n",
    "        print(len(cnt))\n",
    "        return next(iter(cnt)) # next input data set, or raises StopIteration when EOF is hit.\n",
    "    elif df.empty or (not attribute_names):\n",
    "        return default_class  \n",
    "    else:      \n",
    "        default_class = max(cnt.keys()) \n",
    "        gainz = [information_gain(df, attr, target_attribute_name) for attr in attribute_names] \n",
    "        print(\"Gain=\",gainz)\n",
    "        index_of_max = gainz.index(max(gainz)) \n",
    "        best_attr = attribute_names[index_of_max]\n",
    "        print(\"Best Attribute:\",best_attr)\n",
    "        tree = {best_attr:{}} \n",
    "        remaining_attribute_names = [i for i in attribute_names if i != best_attr]\n",
    "        \n",
    "        \n",
    "        for attr_val, data_subset in df.groupby(best_attr):\n",
    "            subtree = id3(data_subset,\n",
    "                        target_attribute_name,\n",
    "                        remaining_attribute_names,\n",
    "                        default_class)\n",
    "            tree[best_attr][attr_val] = subtree\n",
    "        return tree\n",
    "\n",
    "\n",
    "attribute_names = list(df_tennis.columns)\n",
    "print(\"List of Attributes:\", attribute_names) \n",
    "attribute_names.remove('PlayTennis')  \n",
    "print(\"Predicting Attributes:\", attribute_names)\n",
    "\n",
    "# Run Algorithm:\n",
    "from pprint import pprint\n",
    "tree = id3(df_tennis,'PlayTennis',attribute_names)\n",
    "print(\"\\n\\nThe Resultant Decision Tree is :\\n\")\n",
    "pprint(tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.Non-parametric Locally Weighted Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def kernel(point, xmat, k):\n",
    "    m, n = np.shape(xmat)\n",
    "    weights = np.mat(np.eye((m)))\n",
    "    for j in range(m):\n",
    "        diff = point - X[j]\n",
    "        weights[j,j] = np.exp(diff*diff.T/(-2.0*k**2))\n",
    "    return weights\n",
    "\n",
    " \n",
    "def localWeight(point, xmat, ymat, k):\n",
    "    wei = kernel(point, xmat, k)\n",
    "    W = (X.T*(wei*X)).I*(X.T*(wei*ymat.T))\n",
    "    return W\n",
    "\n",
    "def localWeightRegression(xmat, ymat, k):\n",
    "    m, n = np.shape(xmat)\n",
    "    ypred = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        ypred[i] = xmat[i] * localWeight(xmat[i], xmat,ymat, k)\n",
    "    return ypred\n",
    "\n",
    "def graphPlot(X, ypred):\n",
    "    sortindex = X[:,1].argsort(0)\n",
    "    xsort = X[sortindex][:,0]\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.scatter(bill, tip, color='green')\n",
    "    ax.plot(xsort[:,1], ypred[sortindex], color = 'red', linewidth = 5)\n",
    "    plt.xlabel('Total bill')\n",
    "    plt.ylabel('Tip')\n",
    "    plt.show()\n",
    "\n",
    "data = pd.read_csv('data9.csv')\n",
    "bill = np.array(data.total_bill)\n",
    "tip = np.array(data.tip)\n",
    "\n",
    "mbill = np.mat(bill)\n",
    "mtip = np.mat(tip)\n",
    "m = np.shape(mbill)[1]\n",
    "one = np.mat(np.ones(m))\n",
    "X = np.hstack((one.T, mbill.T))\n",
    "\n",
    "ypred = localWeightRegression(X, mtip, 5)\n",
    "graphPlot(X, ypred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.Na√Øve Bayesian classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, random, math\n",
    "\n",
    "\n",
    "def loadcsv(filename):\n",
    "    lines = csv.reader(open(filename, \"r\"))\n",
    "    dataset = list(lines)\n",
    "    # print(dataset)\n",
    "    # print(len(dataset))\n",
    "    for i in range(len(dataset)):\n",
    "        # print(dataset[i])\n",
    "        dataset[i] = [float(x) for x in dataset[i]]\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def splitDataset(dataset, splitRatio):\n",
    "    trainSize = int(len(dataset) * splitRatio)\n",
    "    print(trainSize)\n",
    "    trainSet = []\n",
    "    copy = list(dataset)\n",
    "    while len(trainSet) < trainSize:\n",
    "        index = random.randrange(len(copy))\n",
    "        trainSet.append(copy.pop(index))\n",
    "    return [trainSet, copy]\n",
    "\n",
    "\n",
    "def seperateByClass(dataset):\n",
    "    seperated = {}\n",
    "    for i in range(len(dataset)):\n",
    "        vector = dataset[i]\n",
    "        # print(vector)\n",
    "        if (vector[-1] not in seperated):\n",
    "            seperated[vector[-1]] = []\n",
    "        seperated[vector[-1]].append(vector)\n",
    "    return seperated\n",
    "\n",
    "\n",
    "def mean(numbers):\n",
    "    return sum(numbers) / float(len(numbers))\n",
    "\n",
    "\n",
    "def stddev(numbers):\n",
    "    avg = mean(numbers)\n",
    "    variance = sum([pow(x - avg, 2) for x in numbers]) / float(len(numbers) - 1)\n",
    "    return math.sqrt(variance)\n",
    "\n",
    "\n",
    "def summarize(dataset):\n",
    "    summaries = [(mean(attribute), stddev(attribute)) for attribute in zip(*dataset)]\n",
    "\n",
    "    del summaries[-1]\n",
    "    return summaries\n",
    "\n",
    "\n",
    "def summarizeByClass(dataset):\n",
    "    seperated = seperateByClass(dataset)\n",
    "    # print(\"Separated\", seperated)\n",
    "    summaries = {}\n",
    "    for classValue, instances in seperated.items():\n",
    "        summaries[classValue] = summarize(instances)\n",
    "    return summaries\n",
    "\n",
    "\n",
    "def calculateProbablity(x, mean, stddev):\n",
    "    exponent = math.exp(-(math.pow(x - mean, 2) / (2 * math.pow(stddev, 2))))\n",
    "    return (1 / (math.sqrt(2 * math.pi) * stddev)) * exponent\n",
    "\n",
    "\n",
    "def calculateClassProbablities(summaries, inputVector):\n",
    "    probabilities = {}\n",
    "    for classValue, classSummaries in summaries.items():\n",
    "        probabilities[classValue] = 1\n",
    "        for i in range(len(classSummaries)):\n",
    "            mean, stddev = classSummaries[i]\n",
    "            x = inputVector[i]\n",
    "            print(\"**\", mean, stddev, x)\n",
    "            probabilities[classValue] *= calculateProbablity(x, mean, stddev)\n",
    "            print(\"Individual probability\", probabilities[classValue])\n",
    "    return probabilities\n",
    "\n",
    "\n",
    "def predict(summaries, inputVector):\n",
    "    probabilities = calculateClassProbablities(summaries, inputVector)\n",
    "    print(\"@@@@\", probabilities)\n",
    "    bestLabel, bestProb = None, -1\n",
    "    for classValue, probability in probabilities.items():\n",
    "        if bestLabel is None or probability > bestProb:\n",
    "            bestProb = probability\n",
    "            bestLabel = classValue\n",
    "    return bestLabel\n",
    "\n",
    "\n",
    "def getPredictions(summaries, testSet):\n",
    "    predictions = []\n",
    "    for i in range(len(testSet)):\n",
    "        result = predict(summaries, testSet[i])\n",
    "        predictions.append(result)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def getAccuracy(testSet, predictions):\n",
    "    correct = 0\n",
    "    for i in range(len(testSet)):\n",
    "        if (testSet[i][-1] == predictions[i]):\n",
    "            correct += 1\n",
    "    return (correct / float(len(testSet))) * 100.0\n",
    "\n",
    "\n",
    "def NaiveBayes():\n",
    "    filename = 'data6.csv'\n",
    "    splitRatio = 0.8\n",
    "    dataset = loadcsv(filename)\n",
    "\n",
    "    print(\"\\nThe length of the Data Set :\", len(dataset))\n",
    "    print(\"\\nThe Data Set Splitting into Training and Testing \\n\")\n",
    "    trainingSet, testSet = splitDataset(dataset, splitRatio)\n",
    "\n",
    "    print(\"\\nNumber of Rows in Training Set:{0} rows\".format(len(trainingSet)))\n",
    "    print(\"\\nNumber of Rows in Testing Set:{0} rows\".format(len(testSet)))\n",
    "    print(\"\\nFirst Five Rows of Training Set:\\n\")\n",
    "    for i in range(0, 5):\n",
    "        print(trainingSet[i], \"\\n\")\n",
    "    print(\"\\nFirst Five Rows of Testing Set:\\n\")\n",
    "    for i in range(0, 3):\n",
    "        print(testSet[i], \"\\n\")\n",
    "\n",
    "    summaries = summarizeByClass(trainingSet)\n",
    "    print(\"\\nModel Summaries:\\n\", summaries)\n",
    "\n",
    "    predictions = getPredictions(summaries, testSet)\n",
    "    print(\"\\n Predictions:\\n\",predictions)\n",
    "\n",
    "    accuracy = getAccuracy(testSet, predictions)\n",
    "    print(\"\\nAccuracy: {0}%\".format(accuracy))\n",
    "import csv,random,math\n",
    "NaiveBayes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.k-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import datasets\n",
    "\n",
    "\"\"\"\n",
    "Iris Plants Dataset, dataset contains 150 (50 in each of three classes)\n",
    "Number of Attributes: 4 numeric, predictive attributes and the Class\n",
    "\"\"\"\n",
    "iris=datasets.load_iris()\n",
    "\n",
    "\"\"\" \n",
    "The x variable contains the first four columns of the dataset \n",
    "(i.e. attributes) while y contains the labels.\n",
    "\"\"\"\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "print ('sepal-length', 'sepal-width', 'petal-length', 'petal-width')\n",
    "print(x)\n",
    "print('class: 0-Iris-Setosa, 1- Iris-Versicolour, 2- Iris-Virginica')\n",
    "print(y)\n",
    "\n",
    "\"\"\" splits the dataset into 70% train data and 30% test data. This means that \n",
    "out of total 150 records,the training set will contain 105 records and \n",
    "the test set contains 45 of those records \"\"\"\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3)\n",
    "\n",
    "#to Training the model and Nearest nighbors K=5\n",
    "classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "#to make predictions on our test data\n",
    "y_pred=classifier.predict(x_test)\n",
    "\n",
    "\"\"\" For evaluating an algorithm, confusion matrix, precision, recall and \n",
    "f1 score are the most commonly used metrics.\"\"\"\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('Accuracy Metrics')\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.Artificial Neural Network by implementing the Back propagation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "# numpy is commonly used to process number array\n",
    "\n",
    "# Get the data\n",
    "X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float) # Features ( Hrs Slept, Hrs Studied)\n",
    "y = np.array(([92], [86], [89]), dtype=float)       # Labels(Marks obtained)\n",
    "\n",
    "X = X/np.amax(X,axis=0) # maximum of X array longitudinally\n",
    "y = y/100\n",
    "\n",
    "# Sigmoid Function\n",
    "def sigmoid (x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "# Derivative of Sigmoid Function\n",
    "def derivatives_sigmoid(x):\n",
    "    return x * (1 - x)\n",
    "# Variable initialization\n",
    "epoch=5000              #Setting training iterations\n",
    "lr=0.1                  #Setting learning rate (eta)\n",
    "inputlayer_neurons = 2  #number of features in data set\n",
    "hiddenlayer_neurons = 3 #number of hidden layers neurons\n",
    "output_neurons = 1      #number of neurons at output layer\n",
    "# Weight and bias initialization\n",
    "wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons)) # 2x3\n",
    "bh=np.random.uniform(size=(1,hiddenlayer_neurons))                  # 1x3\n",
    "wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))   # 1x1\n",
    "bout=np.random.uniform(size=(1,output_neurons))\n",
    "\n",
    "# Draws a random range of numbers uniformly of dim x*y\n",
    "for i in range(epoch):\n",
    "    \n",
    "    # Forward Propogation\n",
    "    hinp1=np.dot(X,wh)              # Dot product\n",
    "    hinp=hinp1 + bh                 # Add bias\n",
    "    hlayer_act = sigmoid(hinp)      # Activation function\n",
    "    outinp1=np.dot(hlayer_act,wout) \n",
    "    outinp= outinp1+ bout\n",
    "    output = sigmoid(outinp)\n",
    "\n",
    "    #Backpropagation\n",
    "\n",
    "    # Error at Output layer\n",
    "    EO = y-output                           # Error at o/p\n",
    "    outgrad = derivatives_sigmoid(output)   # \n",
    "    d_output = EO* outgrad                  # Errj=Oj(1-Oj)(Tj-Oj)\n",
    "\n",
    "    # Error at Hidden later\n",
    "    EH = d_output.dot(wout.T)               # .T means transpose\n",
    "    hiddengrad = derivatives_sigmoid(hlayer_act) # How much hidden layer wts contributed to error\n",
    "    d_hiddenlayer = EH * hiddengrad\n",
    "    wout += hlayer_act.T.dot(d_output) *lr  # Dotproduct of nextlayererror and currentlayerop\n",
    "    wh += X.T.dot(d_hiddenlayer) *lr\n",
    "    \n",
    "print(\"Input: \\n\" + str(X)) \n",
    "print(\"Actual Output: \\n\" + str(y))\n",
    "print(\"Predicted Output: \\n\" ,output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.EM Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn.mixture import GaussianMixture \n",
    "from sklearn.cluster import KMeans \n",
    "data = pd.read_csv('data/ex.csv') \n",
    "f1 = data['V1'].values \n",
    "f2 = data['V2'].values \n",
    "X = np.array(list(zip(f1, f2))) \n",
    "print(\"x: \", X) \n",
    "print('Graph for whole dataset') \n",
    "plt.scatter(f1, f2, c='black') # size can be set by adding s=size as param \n",
    "plt.show() \n",
    "kmeans = KMeans(2) \n",
    "labels = kmeans.fit(X).predict(X) \n",
    "print(\"labels for kmeans:\", labels) \n",
    "print('Graph using Kmeans Algorithm') \n",
    "plt.scatter(f1, f2, c=labels) \n",
    "centroids = kmeans.cluster_centers_ \n",
    "print(\"centroids:\", centroids) \n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], marker='*', c='red') \n",
    "plt.show() \n",
    "gmm = GaussianMixture(2) \n",
    "labels = gmm.fit(X).predict(X) \n",
    "print(\"Labels for GMM: \", labels) \n",
    "print('Graph using EM Algorithm') \n",
    "plt.scatter(f1, f2, c=labels) \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
